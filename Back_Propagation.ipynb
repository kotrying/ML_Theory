{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPQeJTKRfqAAMNuo1/a/KBi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kotrying/ML_Theory/blob/main/Back_Propagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "jDMU2ayaQWYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 中間層\n",
        "class MiddleLayer:\n",
        "    def __init__(self, n_upper, n):  # 初期設定\n",
        "        self.w = wb_width * np.random.randn(n_upper, n)  # 重み（行列）\n",
        "        self.b = wb_width * np.random.randn(n)  # バイアス（ベクトル）\n",
        "\n",
        "    def forward(self, x):  # 順伝播\n",
        "        self.x = x\n",
        "        u = np.dot(x, self.w) + self.b\n",
        "        self.y = 1/(1+np.exp(-u))  # シグモイド関数\n",
        "    \n",
        "    def backward(self, grad_y):  # 逆伝播\n",
        "        delta = grad_y * (1-self.y)*self.y  # シグモイド関数の微分\n",
        "        \n",
        "        self.grad_w = np.dot(self.x.T, delta)\n",
        "        self.grad_b = np.sum(delta, axis=0)\n",
        "        \n",
        "        self.grad_x = np.dot(delta, self.w.T) \n",
        "        \n",
        "    def update(self, eta):  # 重みとバイアスの更新\n",
        "        self.w -= eta * self.grad_w\n",
        "        self.b -= eta * self.grad_b"
      ],
      "metadata": {
        "id": "EmYPsFPwQToM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 出力層\n",
        "class OutputLayer:\n",
        "    def __init__(self, n_upper, n):  # 初期設定\n",
        "        self.w = wb_width * np.random.randn(n_upper, n)  # 重み（行列）\n",
        "        self.b = wb_width * np.random.randn(n)  # バイアス（ベクトル）\n",
        "    \n",
        "    def forward(self, x):  # 順伝播\n",
        "        self.x = x\n",
        "        u = np.dot(x, self.w) + self.b\n",
        "        self.y = u  # 恒等関数\n",
        "    \n",
        "    def backward(self, t):  # 逆伝播\n",
        "        delta = self.y - t\n",
        "        \n",
        "        self.grad_w = np.dot(self.x.T, delta)\n",
        "        self.grad_b = np.sum(delta, axis=0)\n",
        "        \n",
        "        self.grad_x = np.dot(delta, self.w.T) \n",
        "\n",
        "    def update(self, eta):  # 重みとバイアスの更新\n",
        "        self.w -= eta * self.grad_w\n",
        "        self.b -= eta * self.grad_b"
      ],
      "metadata": {
        "id": "sT_oMkUwQQnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xLYR7upPJL6"
      },
      "outputs": [],
      "source": [
        "# Back Propagation（誤差逆伝搬）の実装\n",
        "\n",
        "# 各層の初期化\n",
        "# クラスからインスタンス化する\n",
        "middle_layer = MiddleLayer(n_in, n_mid) # （入力層のニューロン数、中間層のニューロン数）\n",
        "output_layer = OutputLayer(n_mid, n_out) # （中間層のニューロン数、出力層のニューロン数）\n",
        "\n",
        "# 学習\n",
        "# エポック数分の学習を実行\n",
        "for i in range(epoch):\n",
        "\n",
        "    # 入力データの数分の要素を持つインデックス行列を作成\n",
        "    index_random = np.arange(n_data)\n",
        "    # インデックスをランダムにシャッフル\n",
        "    np.random.shuffle(index_random)\n",
        "    \n",
        "    # 結果の表示用：誤差、入力、出力を保存し、後で学習の結果を確認する\n",
        "    total_error = 0 # 誤差の初期値：各学習ステップで計算される入力と出力との誤差を加算して\n",
        "    plot_x = [] # 各学習ステップごとに使用した入力\n",
        "    plot_y = [] # 各学習ステップごとに得られた出力\n",
        "    \n",
        "    # インデックス行列から順にインデックスを取り出す\n",
        "    for idx in index_random:\n",
        "        \n",
        "        x = input_data[idx:idx+1]  # 入力：指定のインデックスにおける入力データの値\n",
        "        t = correct_data[idx:idx+1]  # 正解：指定のインデックスにおける正解データの値\n",
        "        \n",
        "        # 順伝播\n",
        "        middle_layer.forward(x.reshape(1, 1)) # 入力を行列に変換\n",
        "        output_layer.forward(middle_layer.y) # 中間層→出力層\n",
        "\n",
        "        # 逆伝播\n",
        "        output_layer.backward(t.reshape(1, 1))  # 正解を行列に変換\n",
        "        middle_layer.backward(output_layer.grad_x) # 出力層→中間層\n",
        "        \n",
        "        # 重みとバイアスの更新\n",
        "        middle_layer.update(eta)\n",
        "        output_layer.update(eta)"
      ]
    }
  ]
}